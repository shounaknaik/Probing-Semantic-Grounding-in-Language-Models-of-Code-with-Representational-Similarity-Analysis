# -*- coding: utf-8 -*-
"""Extracting_embeddings_Code_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6MtctPwrpJNCEK1vCLbHsHsC7dY6sLw

# Setup
"""

# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle

import sys
sys.path.append('..')

from model_embeddings import Model


import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

# load metadata
info_df = pd.read_csv('../../descriptions.csv')

with open('../../test_problems.pkl','rb') as f:
    test_problems=pickle.load(f)


pid_li=[]
for i in range(len(test_problems)):
    pid_li.append(test_problems[i].split('/')[-1].split('.')[0])

def get_submission_id(df,pid):
    
    li=[]
    for _,row in df.iterrows():
       sid=row['submission_id']
       st=f'/home/rahul/probing_codeBERT/Project_CodeNet/data/{pid}/Go/{sid}.go'
       li.append(st)

    return li


accepted_li=[]
semantic_li=[]
# compile_li=[]

for i,path in tqdm(enumerate(test_problems),total=len(test_problems)):

    df_path=f'/home/rahul/probing_codeBERT/Project_CodeNet/metadata/{pid_li[i]}.csv'

    df=pd.read_csv(df_path)


    acc_df=(df.loc[(df['status']=='Accepted') & (df['language']=='Go')])
    WA_df=(df.loc[(df['status']=='Wrong Answer') & (df['language']=='Go')])
    # RE_df=(df.loc[(df['status']=='Runtime Error') & (df['language']=='Go')])
    # CE_df=(df.loc[(df['status']=='Compile Error') & (df['language']=='Go')])

    accepted_li.extend(get_submission_id(acc_df,pid_li[i]))
    semantic_li.extend(get_submission_id(WA_df,pid_li[i]))
    # semantic_li.extend(get_submission_id(RE_df,pid_li[i]))
    # compile_li.extend(get_submission_id(CE_df,pid_li[i]))




tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")

checkpoint_prefix = './checkpoint-best-mrr/model.bin'
model_path=checkpoint_prefix

model=Model(encoder=model,tokenizer=tokenizer)

state_dict = torch.load(model_path)


model.load_state_dict(state_dict)

model.eval()
model.to("cuda:0")




##making layer wise directories

import os

if not os.path.exists('./codeBERT_accepted'):

    os.mkdir('./codeBERT_accepted')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_accepted/layer_{}".format(
                    i
                )
            )
        )

if not os.path.exists('./codeBERT_semantic'):

    os.mkdir('./codeBERT_semantic')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_semantic/layer_{}".format(
                    i
                )
            )
        )


# Code-BERT Model

print('Extracting embeddings for Accepted Submissions')

for path in tqdm(accepted_li,total=len(accepted_li)):

    pid=path.split('/')[-3]
    sid=path.split('/')[-1].split('.')[0]
    text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

    with open(path,'r') as f:
        code=f.read()



    code_tokens=tokenizer.tokenize(code)[:256-2]
    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)
    padding_length = 256 - len(code_ids)
    code_ids+=[tokenizer.pad_token_id]*padding_length


      
    nl_tokens=tokenizer.tokenize(text_phrase)[:256-2]
    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]
    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)
    padding_length = 256 - len(nl_ids)
    nl_ids+=[tokenizer.pad_token_id]*padding_length

      # print(type(code_ids))
    nl_ids=torch.Tensor(np.array(nl_ids)).unsqueeze(0)
    code_ids=torch.Tensor(np.array(code_ids)).unsqueeze(0)

    nl_ids=nl_ids.to(torch.int64).to('cuda:0')
    code_ids=code_ids.to(torch.int64).to('cuda:0')

    outputs=model(code_ids,nl_ids)

    # model_input=tokenized_text['input_ids'].to('cuda:0')

    # context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



    for i,layer in enumerate(outputs['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_accepted/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)


print('Extracting embedding for Wrong  Submissions')

for path in tqdm(semantic_li,total=len(semantic_li)):

    pid=path.split('/')[-3]
    sid=path.split('/')[-1].split('.')[0]
    text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

    with open(path,'r') as f:
        code=f.read()


    code_tokens=tokenizer.tokenize(code)[:256-2]
    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)
    padding_length = 256 - len(code_ids)
    code_ids+=[tokenizer.pad_token_id]*padding_length


      
    nl_tokens=tokenizer.tokenize(text_phrase)[:256-2]
    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]
    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)
    padding_length = 256 - len(nl_ids)
    nl_ids+=[tokenizer.pad_token_id]*padding_length

      # print(type(code_ids))
    nl_ids=torch.Tensor(np.array(nl_ids)).unsqueeze(0)
    code_ids=torch.Tensor(np.array(code_ids)).unsqueeze(0)

    nl_ids=nl_ids.to(torch.int64).to('cuda:0')
    code_ids=code_ids.to(torch.int64).to('cuda:0')

    outputs=model(code_ids,nl_ids)



    for i,layer in enumerate(outputs['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_semantic/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)



# print('Extracting embeddings for Compilation error  Submissions')

# for path in tqdm(compile_li,total=len(compile_li)):

#     pid=path.split('/')[-3]
#     sid=path.split('/')[-1].split('.')[0]
#     text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

#     with open(path,'r') as f:
#         code=f.read()

#     # tokenized_text = tokenizer(
#     #         text_phrase,
#     #         code,
#     #         add_special_tokens=True,
#     #         return_tensors="pt",
#     #         truncation=True,
#     #     )

#     # model_input=tokenized_text['input_ids'].to('cuda:0')

#     # context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)

#     code_tokens=tokenizer.tokenize(code)[:256-2]
#     code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
#     code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)
#     padding_length = 256 - len(code_ids)
#     code_ids+=[tokenizer.pad_token_id]*padding_length


      
#     nl_tokens=tokenizer.tokenize(text_phrase)[:256-2]
#     nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]
#     nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)
#     padding_length = 256 - len(nl_ids)
#     nl_ids+=[tokenizer.pad_token_id]*padding_length

#       # print(type(code_ids))
#     nl_ids=torch.Tensor(np.array(nl_ids)).unsqueeze(0)
#     code_ids=torch.Tensor(np.array(code_ids)).unsqueeze(0)

#     nl_ids=nl_ids.to(torch.int64).to('cuda:0')
#     code_ids=code_ids.to(torch.int64).to('cuda:0')

#     outputs=model(code_ids,nl_ids)



#     for i,layer in enumerate(outputs['hidden_states']):

#         mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
#         with open(f"./codeBERT_compile/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
#             np.save(f, mean_embedding)









print("Printed immediately.")
# time.sleep(3600)
# print("Printed after 1 hour")


