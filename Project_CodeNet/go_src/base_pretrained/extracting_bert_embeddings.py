# -*- coding: utf-8 -*-
"""Extracting_embeddings_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LTYJ5xpcwCJ1oUnnMJlW7ja9fhwjaKEC

# Setup
"""




# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import time

import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

import os
import pickle

if not os.path.exists('./BERT_embeddings'):
    os.mkdir('./BERT_embeddings')


# load metadata
info_df = pd.read_csv('descriptions.csv')

# print(info_df.columns)

with open('../../test_problems.pkl','rb') as f:
    test_problems=pickle.load(f)

for i in range(len(test_problems)):
    test_problems[i]=test_problems[i].split('/')[-1].split('.')[0]



"""# BERT"""

# load pretrained model/tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
model.eval()
model.to("cuda:0")

# BERT final-layer NL embedding
for _,row in tqdm(info_df.iterrows(), total=len(info_df)):

    pid=row['index']
    phrase=row['Final_description']

    if pid in test_problems:
        tokenized_text = tokenizer(
            phrase, add_special_tokens=True, return_tensors="pt", truncation=True
        )
        tokenized_text=tokenized_text.to("cuda:0")
        with torch.no_grad():
            outputs = model(**tokenized_text, return_dict=True)
            embeddings = outputs["last_hidden_state"]
            mean_embedding = torch.mean(embeddings, 1, True).cpu().detach().numpy()
            with open(f"./BERT_embeddings/{pid}_BERT.npy", 'wb') as f:
                np.save(f, mean_embedding)



print("Printed immediately.")
# time.sleep(3600)
# print("Printed after 1 hour")