# -*- coding: utf-8 -*-
"""Extracting_embeddings_Code_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6MtctPwrpJNCEK1vCLbHsHsC7dY6sLw

# Setup
"""

# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle




import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

import sys
sys.path.append('..')

from model_embeddings import Model

# load metadata
info_df = pd.read_csv('../../descriptions.csv')

with open('../../test_problems.pkl','rb') as f:
    test_problems=pickle.load(f)


pid_li=[]
for i in range(len(test_problems)):
    pid_li.append(test_problems[i].split('/')[-1].split('.')[0])

language=sys.argv[1]

extension_dict={'Java':'java','Python':'py','Go':'go','JavaScript':'js','PHP':'php','Ruby':'rb'}

def get_submission_id(df,pid):
    
    #count=0
    li=[]
    for _,row in df.iterrows():
       sid=row['submission_id']
       st=f'/home/rahul/probing_codeBERT/Project_CodeNet/data/{pid}/{language}/{sid}.{extension_dict[language]}'
       li.append(st)
       # count+=1
       # if count==100:
       #  break


    return li


accepted_li=[]
semantic_li=[]
# compile_li=[]



for i,path in tqdm(enumerate(test_problems),total=len(test_problems)):

    df_path=f'/home/rahul/probing_codeBERT/Project_CodeNet/metadata/{pid_li[i]}.csv'

    df=pd.read_csv(df_path)


    acc_df=(df.loc[(df['status']=='Accepted') & (df['language']==language)])
    WA_df=(df.loc[(df['status']=='Wrong Answer') & (df['language']==language)])
    # RE_df=(df.loc[(df['status']=='Runtime Error') & (df['language']=='Go')])
    # CE_df=(df.loc[(df['status']=='Compile Error') & (df['language']=='Go')])

    accepted_li.extend(get_submission_id(acc_df,pid_li[i]))
    semantic_li.extend(get_submission_id(WA_df,pid_li[i]))
    # semantic_li.extend(get_submission_id(RE_df,pid_li[i]))
    # compile_li.extend(get_submission_id(CE_df,pid_li[i]))




tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")
model.eval()
model.to("cuda:0")

##For Finetuning Format
model_embedding=Model(encoder=model,tokenizer=tokenizer)
model_embedding.eval()
model_embedding.to("cuda:0")
##making layer wise directories

import os

if not os.path.exists('./codeBERT_accepted'):

    os.mkdir('./codeBERT_accepted')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_accepted/layer_{}".format(
                    i
                )
            )
        )

if not os.path.exists('./codeBERT_semantic'):

    os.mkdir('./codeBERT_semantic')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_semantic/layer_{}".format(
                    i
                )
            )
        )

if not os.path.exists('./codeBERT_accepted_finetuned_format'):

    os.mkdir('./codeBERT_accepted_finetuned_format')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_accepted_finetuned_format/layer_{}".format(
                    i
                )
            )
        )

if not os.path.exists('./codeBERT_semantic_finetuned_format'):

    os.mkdir('./codeBERT_semantic_finetuned_format')

    for i in range(1, 14):
        os.mkdir(
            str(
                "./codeBERT_semantic_finetuned_format/layer_{}".format(
                    i
                )
            )
        )

# if not os.path.exists('./codeBERT_compile'):

#     os.mkdir('./codeBERT_compile')

#     for i in range(1, 14):
#         os.mkdir(
#             str(
#                 "./codeBERT_compile/layer_{}".format(
#                     i
#                 )
#             )
#         )

    ##making layer wise directories

# Code-BERT Model

print('Extracting embeddings for Accepted Submissions')

for path in tqdm(accepted_li,total=len(accepted_li)):

    pid=path.split('/')[-3]
    sid=path.split('/')[-1].split('.')[0]
    text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

    with open(path,'r') as f:
        code=f.read()

    # print(text_phrase)
    # print(type(code))

    tokenized_text = tokenizer(
            text_phrase,
            code,
            add_special_tokens=True,
            return_tensors="pt",
            truncation=True,
        )

    model_input=tokenized_text['input_ids'].to('cuda:0')

    context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



    for i,layer in enumerate(context_embeddings['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_accepted/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)


    code_tokens=tokenizer.tokenize(code)[:256-2]
    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)
    padding_length = 256 - len(code_ids)
    code_ids+=[tokenizer.pad_token_id]*padding_length


      
    nl_tokens=tokenizer.tokenize(text_phrase)[:256-2]
    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]
    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)
    padding_length = 256 - len(nl_ids)
    nl_ids+=[tokenizer.pad_token_id]*padding_length

      # print(type(code_ids))
    nl_ids=torch.Tensor(np.array(nl_ids)).unsqueeze(0)
    code_ids=torch.Tensor(np.array(code_ids)).unsqueeze(0)

    nl_ids=nl_ids.to(torch.int64).to('cuda:0')
    code_ids=code_ids.to(torch.int64).to('cuda:0')

    outputs=model_embedding(code_ids,nl_ids)

    # model_input=tokenized_text['input_ids'].to('cuda:0')

    # context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



    for i,layer in enumerate(outputs['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_accepted_finetuned_format/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)


print('Extracting embeddings for Wrong  Submissions')

for path in tqdm(semantic_li,total=len(semantic_li)):

    pid=path.split('/')[-3]
    sid=path.split('/')[-1].split('.')[0]
    text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

    with open(path,'r') as f:
        code=f.read()

    tokenized_text = tokenizer(
            text_phrase,
            code,
            add_special_tokens=True,
            return_tensors="pt",
            truncation=True,
        )

    model_input=tokenized_text['input_ids'].to('cuda:0')

    context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



    for i,layer in enumerate(context_embeddings['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_semantic/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)


    code_tokens=tokenizer.tokenize(code)[:256-2]
    code_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
    code_ids =  tokenizer.convert_tokens_to_ids(code_tokens)
    padding_length = 256 - len(code_ids)
    code_ids+=[tokenizer.pad_token_id]*padding_length


      
    nl_tokens=tokenizer.tokenize(text_phrase)[:256-2]
    nl_tokens =[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]
    nl_ids =  tokenizer.convert_tokens_to_ids(nl_tokens)
    padding_length = 256 - len(nl_ids)
    nl_ids+=[tokenizer.pad_token_id]*padding_length

      # print(type(code_ids))
    nl_ids=torch.Tensor(np.array(nl_ids)).unsqueeze(0)
    code_ids=torch.Tensor(np.array(code_ids)).unsqueeze(0)

    nl_ids=nl_ids.to(torch.int64).to('cuda:0')
    code_ids=code_ids.to(torch.int64).to('cuda:0')

    outputs=model_embedding(code_ids,nl_ids)

    # model_input=tokenized_text['input_ids'].to('cuda:0')

    # context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



    for i,layer in enumerate(outputs['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_semantic_finetuned_format/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
            np.save(f, mean_embedding)



# print('Extracting embeddings for Compilation error  Submissions')

# for path in tqdm(compile_li,total=len(compile_li)):

#     pid=path.split('/')[-3]
#     sid=path.split('/')[-1].split('.')[0]
#     text_phrase=info_df.loc[info_df['index']==pid]['Final_description'].item()

#     with open(path,'r') as f:
#         code=f.read()

#     tokenized_text = tokenizer(
#             text_phrase,
#             code,
#             add_special_tokens=True,
#             return_tensors="pt",
#             truncation=True,
#         )

#     model_input=tokenized_text['input_ids'].to('cuda:0')

#     context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)



#     for i,layer in enumerate(context_embeddings['hidden_states']):

#         mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
#         with open(f"./codeBERT_compile/layer_{i+1}/{pid}_{sid}.npy", 'wb') as f:
#             np.save(f, mean_embedding)









print("Printed immediately.")
# time.sleep(3600)



