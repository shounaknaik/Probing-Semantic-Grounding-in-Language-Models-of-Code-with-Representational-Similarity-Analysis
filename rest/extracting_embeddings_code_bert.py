# -*- coding: utf-8 -*-
"""Extracting_embeddings_Code_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6MtctPwrpJNCEK1vCLbHsHsC7dY6sLw

# Setup
"""

# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import time

import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

# load metadata
info_df = pd.read_json('./test.jsonl', lines=True)


#preprocessing function
req_func_li=[]
for index,row in tqdm(info_df.iterrows(),total=len(info_df)):
  temp='def '+ row['identifier'].split('.')[-1]+ row['parameters']+row['function'].split(':',1)[-1]
  req_func_li.append(temp)

info_df['processed_func']=req_func_li
info_df.head()

# load pretrained model/tokenizer

tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")
model.eval()
model.to("cuda:0")

##making layer wise directories

import os

for i in range(1, 14):
    os.mkdir(
        str(
            "./codeBERT_mean/layer_{}".format(
                i
            )
        )
    )

##making layer wise directories

import os

for i in range(1, 14):
    os.mkdir(
        str(
            "./codeBERT_cls/layer_{}".format(
                i
            )
        )
    )

# Code-BERT Model
index_start=0
for index, row in tqdm(info_df[index_start:].iterrows(), total=len(info_df[index_start:])):
      code = row['processed_func']
      text_phrase = row["identifier"]


      tokenized_text = tokenizer(
            text_phrase,
            code,
            add_special_tokens=True,
            return_tensors="pt",
            truncation=True,
        )
      # print((tokenized_text['input_ids']))
      model_input=tokenized_text['input_ids'].to('cuda:0')

      # tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]
  
  
  

      context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)
      # print(context_embeddings)
      # print(context_embeddings['last_hidden_state'])
      

      for i,layer in enumerate(context_embeddings['hidden_states']):

        mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        with open(f"./codeBERT_mean/layer_{i+1}/line_{index}_{i+1}_codeBERT.npy", 'wb') as f:
            np.save(f, mean_embedding)


        cls_embedding=layer[:,0,:].detach().cpu().numpy()
        with open(f"./codeBERT_cls/layer_{i+1}/line_{index}_{i+1}_codeBERT_cls.npy", 'wb') as f:
            np.save(f, cls_embedding)





print("Printed immediately.")
time.sleep(3600)
print("Printed after 1 hour")


