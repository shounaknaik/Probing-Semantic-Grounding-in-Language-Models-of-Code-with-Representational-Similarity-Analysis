# -*- coding: utf-8 -*-
"""Extracting_embeddings_Code_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k6MtctPwrpJNCEK1vCLbHsHsC7dY6sLw

# Setup
"""

# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import time

import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

# load metadata
info_df = pd.read_json('./dataset/test.jsonl', lines=True)


#preprocessing function
req_func_li=[]
for index,row in tqdm(info_df.iterrows(),total=len(info_df)):
  temp='def '+ row['identifier'].split('.')[-1]+ row['parameters']+row['function'].split(':',1)[-1]
  req_func_li.append(temp)

info_df['processed_func']=req_func_li
# info_df.head()

# load pretrained model/tokenizer

tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModel.from_pretrained("microsoft/codebert-base")
model.eval()
model.to("cuda:0")

##making layer wise directories

# import os

# os.mkdir(str('./codeBERT_mean_updated'))

# for i in range(1, 14):
#     os.mkdir(
#         str(
#             "./codeBERT_mean_updated/layer_{}".format(
#                 i
#             )
#         )
#     )

##making layer wise directories

import os

os.mkdir(str('./codeBERT_cls_updated'))

for i in range(1, 14):
    os.mkdir(
        str(
            "./codeBERT_cls_updated/layer_{}".format(
                i
            )
        )
    )

def codebert_tokenizer(nl, code, tokenizer, max_len=512):
    # taken from codeBERT repo
    nl_tokens=tokenizer.tokenize(nl)
    code_tokens=tokenizer.tokenize(code)
    tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]

    # adding extra code for padding / truncation
    if len(tokens)>max_len:
        tokens = tokens[:max_len]
    else:
        tokens += [tokenizer.pad_token]*(max_len-len(tokens))

    # converting subword tokens to their integer IDs
    token_ids=tokenizer.convert_tokens_to_ids(tokens)
    return torch.tensor(token_ids)

# Code-BERT Model
index_start=0
for index, row in tqdm(info_df[index_start:].iterrows(), total=len(info_df[index_start:])):
      code = row['processed_func']
      text_phrase = row["docstring"]

      model_input=codebert_tokenizer(text_phrase,code,tokenizer).unsqueeze(0).to('cuda:0')



      # tokenized_text = tokenizer(
      #       text_phrase,
      #       code,
      #       add_special_tokens=True,
      #       return_tensors="pt",
      #       truncation=True,
      #   )
      # print((tokenized_text['input_ids']))

      # model_input=tokenized_text['input_ids'].to('cuda:0')

  
  
  

      context_embeddings=model(model_input, output_hidden_states=True,return_dict=True)
      
      

      for i,layer in enumerate(context_embeddings['hidden_states']):

        # mean_embedding = torch.mean(layer, 1, True).detach().cpu().numpy()
        # with open(f"./codeBERT_mean_updated/layer_{i+1}/line_{index}_{i+1}_codeBERT.npy", 'wb') as f:
        #     np.save(f, mean_embedding)


        cls_embedding=layer[:,0,:].detach().cpu().numpy()
        with open(f"./codeBERT_cls_updated/layer_{i+1}/line_{index}_{i+1}_codeBERT_cls.npy", 'wb') as f:
            np.save(f, cls_embedding)





print("Printed immediately.")
# time.sleep(360)
# print("Printed after 1 hour")


