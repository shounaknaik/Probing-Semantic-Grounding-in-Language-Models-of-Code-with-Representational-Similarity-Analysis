# -*- coding: utf-8 -*-
"""Extracting_embeddings_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LTYJ5xpcwCJ1oUnnMJlW7ja9fhwjaKEC

# Setup
"""




# imports
import numpy as np
import pandas as pd
from tqdm import tqdm
import time

import torch
from transformers import AutoTokenizer
from transformers import AutoConfig, AutoModel

# load metadata
info_df = pd.read_json('./test.jsonl', lines=True)

# load queries
NL_phrases = info_df["docstring_summary"]

"""# BERT"""

# load pretrained model/tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
model.eval()
model.to("cuda:0")

# BERT final-layer NL embedding
for index, phrase in tqdm(enumerate(NL_phrases), total=len(NL_phrases)):
    tokenized_text = tokenizer(
        phrase, add_special_tokens=True, return_tensors="pt", truncation=True
    )
    tokenized_text=tokenized_text.to("cuda:0")
    with torch.no_grad():
        outputs = model(**tokenized_text, return_dict=True)
        embeddings = outputs["last_hidden_state"]
        a = torch.randn(embeddings.size())
        new_embeddings = embeddings[:, 1:-1, :]
        mean_embedding = torch.mean(new_embeddings, 1, True).cpu().detach().numpy()
        with open(f"./BERT_embeddings/file_{index}_BERT.npy", 'wb') as f:
            np.save(f, mean_embedding)



print("Printed immediately.")
# time.sleep(3600)
print("Printed after 1 hour")